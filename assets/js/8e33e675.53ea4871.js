"use strict";(self.webpackChunkcurator_site=self.webpackChunkcurator_site||[]).push([[7590],{9613:(e,t,r)=>{r.d(t,{Zo:()=>p,kt:()=>m});var a=r(9496);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var c=a.createContext({}),l=function(e){var t=a.useContext(c),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},p=function(e){var t=l(e.components);return a.createElement(c.Provider,{value:t},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=l(r),u=n,m=d["".concat(c,".").concat(u)]||d[u]||h[u]||o;return r?a.createElement(m,i(i({ref:t},p),{},{components:r})):a.createElement(m,i({ref:t},p))}));function m(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[d]="string"==typeof e?e:n,i[1]=s;for(var l=2;l<o;l++)i[l]=r[l];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}u.displayName="MDXCreateElement"},9508:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var a=r(7263),n=(r(9496),r(9613));const o={},i="Recipes",s={unversionedId:"recipes",id:"recipes",title:"Recipes",description:"Curator implements all the recipes listed on the ZooKeeper recipes doc (except two phase commit). Click on the recipe name below for detailed documentation.",source:"@site/docs/recipes.md",sourceDirName:".",slug:"/recipes",permalink:"/docs/recipes",draft:!1,editUrl:"https://github.com/apache/curator-site/tree/main/docs/recipes.md",tags:[],version:"current",lastUpdatedBy:"dependabot[bot]",lastUpdatedAt:1718429306,formattedLastUpdatedAt:"Jun 15, 2024",frontMatter:{},sidebar:"docs",previous:{title:"Examples",permalink:"/docs/examples"},next:{title:"Leader Latch",permalink:"/docs/recipes-leader-latch"}},c={},l=[{value:"Elections",id:"elections",level:2},{value:"Locks",id:"locks",level:2},{value:"Barriers",id:"barriers",level:2},{value:"Counters",id:"counters",level:2},{value:"Caches",id:"caches",level:2},{value:"Nodes/Watches",id:"nodeswatches",level:2},{value:"Queues",id:"queues",level:2}],p={toc:l},d="wrapper";function h(e){let{components:t,...r}=e;return(0,n.kt)(d,(0,a.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"recipes"},"Recipes"),(0,n.kt)("p",null,"Curator implements all the recipes listed on the ZooKeeper recipes doc (except two phase commit). Click on the recipe name below for detailed documentation."),(0,n.kt)("admonition",{type:"caution"},(0,n.kt)("p",{parentName:"admonition"},"Curator recipes will automatically create parent nodes of paths given to the recipe as ",(0,n.kt)("inlineCode",{parentName:"p"},"CreateMode.CONTAINER"),". Also, see ",(0,n.kt)("a",{parentName:"p",href:"/docs/tech-note-07"},"Tech Note 7"),' regarding "Curator Recipes Own Their ZNode/Paths".')),(0,n.kt)("h2",{id:"elections"},"Elections"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-leader-latch"},"Leader Latch"),' - In distributed computing, leader election is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are unaware which node will serve as the "leader," or coordinator, of the task. After a leader election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task leader.'),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-leader-election"},"Leader Election")," - Initial Curator leader election recipe."),(0,n.kt)("h2",{id:"locks"},"Locks"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-shared-reentrant-lock"},"Shared Reentrant Lock")," - Fully distributed locks that are globally synchronous, meaning at any snapshot in time no two clients think they hold the same lock."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-shared-lock"},"Shared Lock")," - Similar to Shared Reentrant Lock but not reentrant."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-shared-reentrant-read-write-lock"},"Shared Reentrant Read Write Lock")," - A re-entrant read/write mutex that works across JVMs. A read write lock maintains a pair of associated locks, one for read-only operations and one for writing. The read lock may be held simultaneously by multiple reader processes, so long as there are no writers. The write lock is exclusive."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-shared-semaphore"},"Shared Semaphore"),' - A counting semaphore that works across JVMs. All processes in all JVMs that use the same lock path will achieve an inter-process limited set of leases. Further, this semaphore is mostly "fair" - each user will get a lease in the order requested (from ZK\'s point of view).'),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-multi-shared-lock"},"Multi Shared Lock")," - A container that manages multiple locks as a single entity. When acquire() is called, all the locks are acquired. If that fails, any paths that were acquired are released. Similarly, when release() is called, all locks are released (failures are ignored)."),(0,n.kt)("h2",{id:"barriers"},"Barriers"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-barrier"},"Barrier")," - Distributed systems use barriers to block processing of a set of nodes until a condition is met at which time all the nodes are allowed to proceed."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-double-barrier"},"Double Barrier")," - Double barriers enable clients to synchronize the beginning and the end of a computation. When enough processes have joined the barrier, processes start their computation and leave the barrier once they have finished."),(0,n.kt)("h2",{id:"counters"},"Counters"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-shared-counter"},"Shared Counter")," - Manages a shared integer. All clients watching the same path will have the up-to-date value of the shared integer (considering ZK's normal consistency guarantees)."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-distributed-atomic-long"},"Distributed Atomic Long")," - A counter that attempts atomic increments. It first tries using optimistic locking. If that fails, an optional InterProcessMutex is taken. For both optimistic and mutex, a retry policy is used to retry the increment."),(0,n.kt)("h2",{id:"caches"},"Caches"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-curator-cache"},"Curator Cache")," - A utility that attempts to keep the data from a node locally cached. Optionally the entire tree of children below the node can also be cached. Will respond to update/create/delete events, pull down the data, etc. You can register listeners that will get notified when changes occur."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-path-cache"},"Path Cache")," - (For preZooKeeper 3.6.x) A Path Cache is used to watch a ZNode. Whenever a child is added, updated or removed, the Path Cache will change its state to contain the current set of children, the children's data and the children's state. Path caches in the Curator Framework are provided by the PathChildrenCache class. Changes to the path are passed to registered PathChildrenCacheListener instances."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-node-cache"},"Node Cache")," - (For preZooKeeper 3.6.x) A utility that attempts to keep the data from a node locally cached. This class will watch the node, respond to update/create/delete events, pull down the data, etc. You can register a listener that will get notified when changes occur."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-tree-cache"},"Tree Cache")," - (For preZooKeeper 3.6.x) A utility that attempts to keep all data from all children of a ZK path locally cached. This class will watch the ZK path, respond to update/create/delete events, pull down the data, etc. You can register a listener that will get notified when changes occur."),(0,n.kt)("h2",{id:"nodeswatches"},"Nodes/Watches"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-persistent-watcher"},"Persistent Recursive Watcher")," - A managed persistent recursive watcher. The watch will be managed such that it stays set through connection lapses, etc."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-persistent-node"},"Persistent Node")," - A node that attempts to stay present in ZooKeeper, even through connection and session interruptions."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-persistent-ttl-node"},"Persistent TTL Node")," - Useful when you need to create a TTL node but don't want to keep it alive manually by periodically setting data."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-group-member"},"Group Member")," - Group membership management. Adds this instance into a group and keeps a cache of"),(0,n.kt)("h2",{id:"queues"},"Queues"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-distributed-queue"},"Distributed Queue")," - An implementation of the Distributed Queue ZK recipe. Items put into the queue are guaranteed to be ordered (by means of ZK's PERSISTENTSEQUENTIAL node). If a single consumer takes items out of the queue, they will be ordered FIFO. If ordering is important, use a LeaderSelector to nominate a single consumer."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-distributed-id-queue"},"Distributed ID Queue")," - A version of DistributedQueue that allows IDs to be associated with queue items. Items can then be removed from the queue if needed."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-distributed-priority-queue"},"Distributed Priority Queue")," - An implementation of the Distributed Priority Queue ZK recipe."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-distributed-delay-queue"},"Distributed Delay Queue")," - An implementation of a Distributed Delay Queue."),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"/docs/recipes-simple-distributed-queue"},"Simple Distributed Queue")," - A drop-in replacement for the DistributedQueue that comes with the ZK distribution."))}h.isMDXComponent=!0}}]);